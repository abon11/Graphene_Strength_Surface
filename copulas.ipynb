{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d289cf39",
   "metadata": {},
   "source": [
    "## This notebook is me playing around with the `copulas` package in python. The short answer here is that it does not do what we want it to do, so we switched to using `R`.\n",
    "\n",
    "Note that we no longer use copulas, and this is outdated for the current method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a28a9",
   "metadata": {},
   "source": [
    "We can generate a PDF using Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708667d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"drucker_prager_params.csv\")\n",
    "\n",
    "data = np.vstack([df[\"alpha\"], df[\"k\"]])  # shape (2, N)\n",
    "kde = gaussian_kde(data, bw_method='scott')\n",
    "\n",
    "print(\"Bandwidth factor:\", kde.factor)\n",
    "print(\"Covariance matrix:\\n\", kde.covariance)\n",
    "bandwidth_matrix = kde.factor ** 2 * np.cov(data)\n",
    "\n",
    "alpha_grid = np.linspace(min(df[\"alpha\"]) - 0.01, max(df[\"alpha\"]) + 0.01, 100)\n",
    "k_grid = np.linspace(0, max(df[\"k\"]) + 5, 100)  # ensure k > 0\n",
    "A, K = np.meshgrid(alpha_grid, k_grid)\n",
    "\n",
    "# Flatten and stack grid for KDE evaluation\n",
    "coords = np.vstack([A.ravel(), K.ravel()])\n",
    "Z = kde(coords).reshape(A.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(A, K, Z, levels=30, cmap='viridis')\n",
    "# plt.scatter(df[\"alpha\"], df[\"k\"], s=10, c='white', alpha=0.6)\n",
    "plt.colorbar(label='Density')\n",
    "plt.xlabel(r'$\\alpha$')\n",
    "plt.ylabel(r'$k$')\n",
    "plt.title(rf'Joint Density of $(\\alpha, k)$ using Gaussian KDE, $h$={kde.factor:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943ff5a",
   "metadata": {},
   "source": [
    "The problem with this is that it doesn't respect the bounds. Instead, we can use copulas, where we can define each marginal distribution to one that respects the physical bounds of the problem, then model the relationship between the two variables with a copula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3455ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copulas.univariate import BetaUnivariate, GammaUnivariate\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "from copulas.visualization import compare_2d\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"drucker_prager_params.csv\")\n",
    "data = df.drop(columns=\"Seed\")\n",
    "\n",
    "dist = GaussianMultivariate(distribution={\n",
    "    \"alpha\": BetaUnivariate(),\n",
    "    \"k\": GammaUnivariate()\n",
    "})\n",
    "\n",
    "dist.fit(data)\n",
    "sampled = dist.sample(1000)\n",
    "\n",
    "compare_2d(data, sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2d594",
   "metadata": {},
   "source": [
    "To import the bounds properly, we must scale our data such that $[-\\sqrt(3)/3, \\sqrt(3)/3] \\rightarrow [0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548057ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copulas.univariate import BetaUnivariate, GammaUnivariate\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "from copulas.visualization import compare_1d\n",
    "from copulas.visualization import compare_2d\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"drucker_prager_params.csv\")\n",
    "data = df.drop(columns=\"Seed\")\n",
    "\n",
    "alpha_lower = -np.sqrt(3)/3\n",
    "alpha_upper = np.sqrt(3)/3\n",
    "\n",
    "# Rescale alpha to [0,1]\n",
    "data_scaled = data.copy()\n",
    "data_scaled[\"alpha\"] = np.clip((data[\"alpha\"] - alpha_lower) / (alpha_upper - alpha_lower), 0.01, 0.99)\n",
    "\n",
    "# Fit copula with scaled alpha\n",
    "dist = GaussianMultivariate(distribution={\n",
    "    \"alpha\": BetaUnivariate(),\n",
    "    \"k\": GammaUnivariate()\n",
    "})\n",
    "\n",
    "dist.fit(data_scaled)\n",
    "sampled_scaled = dist.sample(1000)\n",
    "\n",
    "# Unscale alpha back to original domain\n",
    "sampled = sampled_scaled.copy()\n",
    "sampled[\"alpha\"] = sampled_scaled[\"alpha\"] * (alpha_upper - alpha_lower) + alpha_lower\n",
    "\n",
    "unscaled_beginning_data = data_scaled.copy()\n",
    "unscaled_beginning_data[\"alpha\"] = unscaled_beginning_data[\"alpha\"] * (alpha_upper - alpha_lower) + alpha_lower\n",
    "\n",
    "# Visual comparison\n",
    "fig = compare_2d(unscaled_beginning_data, sampled)\n",
    "fig.show()\n",
    "fig = compare_1d(unscaled_beginning_data[\"alpha\"], sampled[\"alpha\"])\n",
    "fig.show()\n",
    "fig = compare_1d(unscaled_beginning_data[\"k\"], sampled[\"k\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c81fb",
   "metadata": {},
   "source": [
    "This looks good so far. Let's plot the actual PDF, not just samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e290da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define physical ranges\n",
    "x = np.linspace(-0.15, 0.15, 1000)\n",
    "y = np.linspace(24, 53, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Flatten grid into DataFrame (and scale alpha)\n",
    "grid = pd.DataFrame({\n",
    "    \"alpha\": np.clip((X.ravel() - alpha_lower) / (alpha_upper - alpha_lower), 0.01, 0.99),\n",
    "    \"k\": Y.ravel()\n",
    "})\n",
    "\n",
    "# Evaluate PDF from copula model (returns array)\n",
    "Z = dist.probability_density(grid)\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(X, Y, Z, cmap=\"viridis\", levels=50)\n",
    "plt.contour(X, Y, Z, levels=[1e-12], colors='red', linewidths=1.5)\n",
    "plt.scatter(unscaled_beginning_data[\"alpha\"], unscaled_beginning_data[\"k\"], c='white', s=2, alpha=0.5)\n",
    "plt.ylim(24, 53)\n",
    "plt.xlim(-0.15, 0.15)\n",
    "\n",
    "plt.colorbar(label='PDF')\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(r\"$k$\")\n",
    "plt.title(\"Estimated Joint PDF from Copula Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50173f",
   "metadata": {},
   "source": [
    "We can extend this to look at each marginal as well, to ensure they are behaving correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import beta, gamma\n",
    "from copulas.univariate import BetaUnivariate, GammaUnivariate\n",
    "from scipy.stats import beta as beta_dist, gamma as gamma_dist\n",
    "\n",
    "\n",
    "# Manually fit marginals on scaled data\n",
    "alpha_scaled = data_scaled[\"alpha\"].values\n",
    "k_data = data_scaled[\"k\"].values\n",
    "\n",
    "data_clipped = data_scaled.copy()\n",
    "data_clipped[\"alpha\"] = data_clipped[\"alpha\"] * (alpha_upper - alpha_lower) + alpha_lower\n",
    "\n",
    "beta_uni = BetaUnivariate()\n",
    "beta_uni.fit(alpha_scaled)\n",
    "beta_params = beta_uni.to_dict()  # {'a': a, 'b': b}\n",
    "\n",
    "gamma_uni = GammaUnivariate()\n",
    "gamma_uni.fit(k_data)\n",
    "gamma_params = gamma_uni.to_dict()  # {'alpha': shape, 'beta': rate}\n",
    "print(beta_params)\n",
    "\n",
    "# 1. Plot alpha\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "plt.hist(data_clipped[\"alpha\"], bins=50, density=True, alpha=0.5,\n",
    "         label=\"Original α\", color=\"skyblue\")\n",
    "\n",
    "x_plot = np.linspace(-0.15, 0.15, 500)\n",
    "\n",
    "# Convert to unit space (to match original fitted domain)\n",
    "x_scaled = (x_plot - alpha_lower) / (alpha_upper - alpha_lower)\n",
    "\n",
    "bp = beta_params\n",
    "gp = gamma_params\n",
    "\n",
    "# Evaluate fitted Beta PDF with loc/scale\n",
    "pdf_alpha = beta_dist.pdf(\n",
    "    x_scaled,\n",
    "    a=bp[\"a\"],\n",
    "    b=bp[\"b\"],\n",
    "    loc=bp[\"loc\"],\n",
    "    scale=bp[\"scale\"]\n",
    ") / (alpha_upper - alpha_lower)  # Chain rule rescaling from unit space\n",
    "\n",
    "plt.plot(x_plot, pdf_alpha, label=\"Fitted Beta PDF\", color=\"black\", lw=2)\n",
    "# plt.plot([0.1, 0.1], [0, 18], color='red', linestyle='--', label='Cutoff')\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylim(0, 17)\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Marginal PDF of α\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# 2. Plot k\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "plt.hist(data_clipped[\"k\"], bins=50, density=True, alpha=0.5,\n",
    "         label=\"Original k\", color=\"salmon\")\n",
    "\n",
    "x_k = np.linspace(min(sampled[\"k\"]), max(sampled[\"k\"]), 500)\n",
    "\n",
    "# Evaluate fitted Gamma PDF with loc/scale\n",
    "pdf_k = gamma_dist.pdf(\n",
    "    x_k,\n",
    "    a=gp[\"a\"],\n",
    "    loc=gp[\"loc\"],\n",
    "    scale=gp[\"scale\"]\n",
    ")\n",
    "\n",
    "plt.plot(x_k, pdf_k, label=\"Fitted Gamma PDF\", color=\"black\", lw=2)\n",
    "plt.xlabel(r\"$k$\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Marginal PDF of k\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6e74d",
   "metadata": {},
   "source": [
    "That PDF looked kind of weird in the corner, specifically the line at which $P=0$. To confirm our bounding is working correctly, we can go back artificially change our upper bound to be $0.1$, which is much closer to our data. This will allow us to see our bounding in action. If we do this and run the copula again (then visualize the PDFs), we see that they do not actually respect the bounds! This is a problem, and it seems to have something to do with the marginal beta distribution itself.\n",
    "\n",
    "Let's use the `copulas` package to model just the marginal beta distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908275b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copulas.univariate import BetaUnivariate, GammaUnivariate\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "from copulas.visualization import compare_1d\n",
    "from copulas.visualization import compare_2d\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"drucker_prager_params.csv\")\n",
    "data = df.drop(columns=\"Seed\")\n",
    "\n",
    "alpha_lower = -np.sqrt(3)/3\n",
    "alpha_upper = 0.1\n",
    "\n",
    "# Rescale alpha to [0,1]\n",
    "data_scaled = data.copy()\n",
    "data_scaled[\"alpha\"] = np.clip((data[\"alpha\"] - alpha_lower) / (alpha_upper - alpha_lower), 0.01, 0.99)\n",
    "data_clipped = data.copy()\n",
    "data_clipped[\"alpha\"] = data_scaled[\"alpha\"] * (alpha_upper - alpha_lower) + alpha_lower\n",
    "# Fit copula with scaled alpha\n",
    "dist = BetaUnivariate()\n",
    "\n",
    "dist.fit(data_scaled[\"alpha\"])\n",
    "sampled_scaled = dist.sample(1000)\n",
    "\n",
    "# Unscale alpha back to original domain\n",
    "sampled = sampled_scaled.copy()\n",
    "sampled = sampled_scaled * (alpha_upper - alpha_lower) + alpha_lower\n",
    "\n",
    "beta_params = dist.to_dict()\n",
    "\n",
    "print(beta_params)\n",
    "\n",
    "# 1. Plot alpha\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "plt.hist(data_clipped[\"alpha\"], bins=50, density=True, alpha=0.5,\n",
    "         label=\"Original α\", color=\"skyblue\")\n",
    "\n",
    "x_plot = np.linspace(-0.15, 0.15, 500)\n",
    "\n",
    "# Convert to unit space (to match original fitted domain)\n",
    "x_scaled = (x_plot - alpha_lower) / (alpha_upper - alpha_lower)\n",
    "\n",
    "bp = beta_params\n",
    "\n",
    "# Evaluate fitted Beta PDF with loc/scale\n",
    "pdf_alpha = beta_dist.pdf(\n",
    "    x_scaled,\n",
    "    a=bp[\"a\"],\n",
    "    b=bp[\"b\"],\n",
    "    loc=bp[\"loc\"],\n",
    "    scale=bp[\"scale\"]\n",
    ") / (alpha_upper - alpha_lower)  # Chain rule rescaling from unit space\n",
    "\n",
    "plt.plot(x_plot, pdf_alpha, label=\"Fitted Beta PDF\", color=\"black\", lw=2)\n",
    "# plt.plot([0.1, 0.1], [0, 18], color='red', linestyle='--', label='Cutoff')\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylim(0, 17)\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Marginal PDF of α\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig = compare_1d(data[\"alpha\"], sampled)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542906c",
   "metadata": {},
   "source": [
    "When we plot the PDF, we can confirm that the distribution does not respect the bound at 0.1. To confirm that this isn't me doing something wrong, let's do the same setup but with `scipy` on our scaled data, instead of `copulas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d851562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta as beta_dist\n",
    "from scipy.stats import beta as scipy_beta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Fit Beta using scipy on scaled data\n",
    "a_sci, b_sci, loc_sci, scale_sci = scipy_beta.fit(data_scaled[\"alpha\"], floc=0, fscale=1)\n",
    "\n",
    "print(\"Scipy Beta Fit:\")\n",
    "print(f\"a: {a_sci:.4f}, b: {b_sci:.4f}, loc: {loc_sci}, scale: {scale_sci}\")\n",
    "\n",
    "# 2. Compare both fitted PDFs on unscaled domain\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "plt.hist(data_clipped[\"alpha\"], bins=50, density=True, alpha=0.5,\n",
    "         label=\"Original α\", color=\"skyblue\")\n",
    "\n",
    "x_plot = np.linspace(-0.15, 0.15, 500)\n",
    "x_scaled = (x_plot - alpha_lower) / (alpha_upper - alpha_lower)\n",
    "\n",
    "# Copulas Beta PDF\n",
    "pdf_copulas = beta_dist.pdf(\n",
    "    x_scaled,\n",
    "    a=bp[\"a\"],\n",
    "    b=bp[\"b\"],\n",
    "    loc=bp[\"loc\"],\n",
    "    scale=bp[\"scale\"]\n",
    ") / (alpha_upper - alpha_lower)\n",
    "\n",
    "# Scipy Beta PDF\n",
    "pdf_scipy = beta_dist.pdf(\n",
    "    x_scaled,\n",
    "    a=a_sci,\n",
    "    b=b_sci,\n",
    "    loc=0,\n",
    "    scale=1\n",
    ") / (alpha_upper - alpha_lower)\n",
    "\n",
    "# R Beta PDF\n",
    "pdf_R = beta_dist.pdf(\n",
    "    x_scaled,\n",
    "    a=35.55397,\n",
    "    b=6.84320,\n",
    "    loc=0,\n",
    "    scale=1\n",
    ") / (alpha_upper - alpha_lower)\n",
    "\n",
    "plt.plot(x_plot, pdf_copulas, label=\"Copulas Beta PDF\", color=\"black\", lw=2)\n",
    "plt.plot(x_plot, pdf_scipy, label=\"Scipy Beta PDF\", color=\"red\", lw=2, linestyle=\"-\", alpha = 0.7)\n",
    "plt.plot(x_plot, pdf_R, label=\"R Beta PDF\", color=\"green\", lw=2, linestyle=\":\")\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.ylim(0, 17)\n",
    "plt.title(\"Comparison of Beta Fits: Copulas vs Scipy vs R for Truncated Data\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eac722",
   "metadata": {},
   "source": [
    "We can see that the `copulas` beta distribution does not come close to the `scipy` distribution. Additionally, we set up the same exact framework in `R`, and the `scipy` and `R` implementations matched perfectly.\n",
    "\n",
    "The final verdict here is that the `copulas` package does not assume the input data for the Beta distribution is $\\in [0, 1]$. Therefore, it learns the shape parameters (as normal), but also the `loc` and `scale` parameters. When the data is $\\in [0, 1]$, `loc = 0` and `scale = 1`. This is a problem because it essentially causes the `copulas` package to stretch the data to best fit the distribution, ruining the bounds we created for it by scaling our data. \n",
    "\n",
    "Currently, the `copuals` package provides no way to set `loc` and `scale` constant beforehand (unlike `scipy`). This means that moving forward, we must use `R`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lammps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
